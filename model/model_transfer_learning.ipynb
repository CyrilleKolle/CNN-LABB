{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-14 14:08:23.180110: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Models to do transfer learning on\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "\n",
    "from model_class import Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_FLOWERS = \"../data/flowers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = model.load_data(PATH_TO_FLOWERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = model.convert_images_column_to_tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(((tf.Tensor([ 81. 103.  28.], shape=(3,), dty...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(((tf.Tensor([30. 35.  3.], shape=(3,), dtype=...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(((tf.Tensor([0. 0. 0.], shape=(3,), dtype=flo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(((tf.Tensor([22. 37. 12.], shape=(3,), dtype=...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(((tf.Tensor([48. 58. 49.], shape=(3,), dtype=...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4312</th>\n",
       "      <td>(((tf.Tensor([249. 252. 106.], shape=(3,), dty...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4313</th>\n",
       "      <td>(((tf.Tensor([79. 88. 92.], shape=(3,), dtype=...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4314</th>\n",
       "      <td>(((tf.Tensor([46. 62. 16.], shape=(3,), dtype=...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4315</th>\n",
       "      <td>(((tf.Tensor([222. 228. 243.], shape=(3,), dty...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4316</th>\n",
       "      <td>(((tf.Tensor([105. 104. 109.], shape=(3,), dty...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4317 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 images  labels\n",
       "0     (((tf.Tensor([ 81. 103.  28.], shape=(3,), dty...       0\n",
       "1     (((tf.Tensor([30. 35.  3.], shape=(3,), dtype=...       0\n",
       "2     (((tf.Tensor([0. 0. 0.], shape=(3,), dtype=flo...       0\n",
       "3     (((tf.Tensor([22. 37. 12.], shape=(3,), dtype=...       0\n",
       "4     (((tf.Tensor([48. 58. 49.], shape=(3,), dtype=...       0\n",
       "...                                                 ...     ...\n",
       "4312  (((tf.Tensor([249. 252. 106.], shape=(3,), dty...       3\n",
       "4313  (((tf.Tensor([79. 88. 92.], shape=(3,), dtype=...       3\n",
       "4314  (((tf.Tensor([46. 62. 16.], shape=(3,), dtype=...       3\n",
       "4315  (((tf.Tensor([222. 228. 243.], shape=(3,), dty...       3\n",
       "4316  (((tf.Tensor([105. 104. 109.], shape=(3,), dty...       3\n",
       "\n",
       "[4317 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encode_labels(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = model.split_target_from_features(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, X_val, y_train, y_test, y_val = model.split_train_test_val(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2762,), (864,), (691,), (2762,), (864,), (691,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, X_val.shape, y_train.shape, y_test.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, X_val = model.reshape_images(X_train, X_test, X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2762, 224, 224, 3), (864, 224, 224, 3), (691, 224, 224, 3))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = tf.keras.applications.vgg19.preprocess_input(X_train)\n",
    "test_images = tf.keras.applications.vgg19.preprocess_input(X_test)\n",
    "val_images = tf.keras.applications.vgg19.preprocess_input(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2762,), (864,), (691,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_test.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)   \n",
    "y_test = to_categorical(y_test)\n",
    "y_val = to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vgg19():\n",
    "    vgg19 = VGG19(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "    vgg19.trainable = False\n",
    "    model = Sequential()\n",
    "    model.add(vgg19)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.15.0'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19 = create_vgg19()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg19 (Functional)          (None, 7, 7, 512)         20024384  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               6422784   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 1285      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 26448453 (100.89 MB)\n",
      "Trainable params: 6424069 (24.51 MB)\n",
      "Non-trainable params: 20024384 (76.39 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg19.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',      \n",
    "    patience=5,             \n",
    "    min_delta=0.001,        \n",
    "    mode='min',             \n",
    "    restore_best_weights=True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "87/87 [==============================] - 352s 4s/step - loss: 4.7809 - accuracy: 0.7487 - val_loss: 0.8640 - val_accuracy: 0.8162\n",
      "Epoch 2/20\n",
      "87/87 [==============================] - 343s 4s/step - loss: 0.2280 - accuracy: 0.9337 - val_loss: 0.8255 - val_accuracy: 0.8538\n",
      "Epoch 3/20\n",
      "87/87 [==============================] - 341s 4s/step - loss: 0.0539 - accuracy: 0.9841 - val_loss: 0.7944 - val_accuracy: 0.8625\n",
      "Epoch 4/20\n",
      "87/87 [==============================] - 319s 4s/step - loss: 0.0171 - accuracy: 0.9953 - val_loss: 0.8231 - val_accuracy: 0.8625\n",
      "Epoch 5/20\n",
      "87/87 [==============================] - 319s 4s/step - loss: 0.0116 - accuracy: 0.9978 - val_loss: 0.8553 - val_accuracy: 0.8611\n",
      "Epoch 6/20\n",
      "87/87 [==============================] - 322s 4s/step - loss: 0.0066 - accuracy: 0.9982 - val_loss: 0.8664 - val_accuracy: 0.8596\n",
      "Epoch 7/20\n",
      "87/87 [==============================] - 320s 4s/step - loss: 0.0036 - accuracy: 0.9989 - val_loss: 0.8714 - val_accuracy: 0.8596\n",
      "Epoch 8/20\n",
      "87/87 [==============================] - 319s 4s/step - loss: 0.0023 - accuracy: 0.9993 - val_loss: 0.8905 - val_accuracy: 0.8625\n"
     ]
    }
   ],
   "source": [
    "history = vgg19.fit(train_images, y_train, epochs=20, validation_data=(val_images, y_val), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2762, 5)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 78s 3s/step - loss: 0.7163 - accuracy: 0.8542\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7162773013114929, 0.8541666865348816]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg19.evaluate(test_images, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fine_tuned_vgg19():\n",
    "    vgg19 = VGG19(include_top=False, weights=\"imagenet\", input_shape=(224, 224, 3))\n",
    "\n",
    "    fine_tune_at = 2\n",
    "\n",
    "    for layer in vgg19.layers[:fine_tune_at]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(vgg19)\n",
    "    model.add(Conv2D(128, (3, 3), activation=\"relu\"))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(\n",
    "        Dense(\n",
    "            128, activation=\"relu\"\n",
    "        )\n",
    "    )\n",
    "    model.add(\n",
    "        Dense(256, activation=\"relu\")\n",
    "    )\n",
    "    model.add(Dense(5, activation=\"softmax\"))\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_vgg19 = create_fine_tuned_vgg19()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg19 (Functional)          (None, 7, 7, 512)         20024384  \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 5, 5, 128)         589952    \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 2, 2, 128)         0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               65664     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 5)                 1285      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20714309 (79.02 MB)\n",
      "Trainable params: 20712517 (79.01 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tuned_vgg19.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "87/87 [==============================] - 994s 11s/step - loss: 2.2385 - accuracy: 0.3204 - val_loss: 1.2803 - val_accuracy: 0.4269\n",
      "Epoch 2/20\n",
      "87/87 [==============================] - 992s 11s/step - loss: 1.2670 - accuracy: 0.4479 - val_loss: 1.3828 - val_accuracy: 0.3806\n",
      "Epoch 3/20\n",
      "87/87 [==============================] - 977s 11s/step - loss: 1.1744 - accuracy: 0.5217 - val_loss: 1.0900 - val_accuracy: 0.5384\n",
      "Epoch 4/20\n",
      "87/87 [==============================] - 977s 11s/step - loss: 1.1243 - accuracy: 0.5264 - val_loss: 1.1174 - val_accuracy: 0.5369\n",
      "Epoch 5/20\n",
      "87/87 [==============================] - 970s 11s/step - loss: 1.0752 - accuracy: 0.5587 - val_loss: 1.0342 - val_accuracy: 0.5861\n",
      "Epoch 6/20\n",
      "87/87 [==============================] - 991s 11s/step - loss: 1.0322 - accuracy: 0.5804 - val_loss: 1.0358 - val_accuracy: 0.5904\n",
      "Epoch 7/20\n",
      "87/87 [==============================] - 1100s 13s/step - loss: 1.0224 - accuracy: 0.5815 - val_loss: 1.0989 - val_accuracy: 0.5499\n",
      "Epoch 8/20\n",
      "87/87 [==============================] - 980s 11s/step - loss: 1.0027 - accuracy: 0.5981 - val_loss: 1.0683 - val_accuracy: 0.5745\n",
      "Epoch 9/20\n",
      "87/87 [==============================] - 974s 11s/step - loss: 0.9741 - accuracy: 0.6035 - val_loss: 1.0101 - val_accuracy: 0.5890\n",
      "Epoch 10/20\n",
      "87/87 [==============================] - 1040s 12s/step - loss: 1.0031 - accuracy: 0.5941 - val_loss: 0.9844 - val_accuracy: 0.6179\n",
      "Epoch 11/20\n",
      "87/87 [==============================] - 1052s 12s/step - loss: 0.9832 - accuracy: 0.6007 - val_loss: 1.0638 - val_accuracy: 0.5282\n",
      "Epoch 12/20\n",
      "87/87 [==============================] - 970s 11s/step - loss: 0.9643 - accuracy: 0.6180 - val_loss: 0.9578 - val_accuracy: 0.6122\n",
      "Epoch 13/20\n",
      "87/87 [==============================] - 970s 11s/step - loss: 0.9192 - accuracy: 0.6430 - val_loss: 0.9764 - val_accuracy: 0.6049\n",
      "Epoch 14/20\n",
      "87/87 [==============================] - 1093s 13s/step - loss: 0.8815 - accuracy: 0.6463 - val_loss: 0.9435 - val_accuracy: 0.6368\n",
      "Epoch 15/20\n",
      "87/87 [==============================] - 1018s 12s/step - loss: 0.8651 - accuracy: 0.6673 - val_loss: 0.9191 - val_accuracy: 0.6671\n",
      "Epoch 16/20\n",
      "87/87 [==============================] - 1002s 12s/step - loss: 0.8323 - accuracy: 0.6857 - val_loss: 0.9787 - val_accuracy: 0.6049\n",
      "Epoch 17/20\n",
      "87/87 [==============================] - 1381s 16s/step - loss: 0.8263 - accuracy: 0.6658 - val_loss: 1.0195 - val_accuracy: 0.6252\n",
      "Epoch 18/20\n",
      "87/87 [==============================] - 1099s 13s/step - loss: 0.8143 - accuracy: 0.6767 - val_loss: 0.9588 - val_accuracy: 0.5876\n",
      "Epoch 19/20\n",
      "87/87 [==============================] - 1310s 15s/step - loss: 0.8268 - accuracy: 0.6720 - val_loss: 0.8561 - val_accuracy: 0.6657\n",
      "Epoch 20/20\n",
      "87/87 [==============================] - 1150s 13s/step - loss: 0.7743 - accuracy: 0.6988 - val_loss: 0.8851 - val_accuracy: 0.6498\n"
     ]
    }
   ],
   "source": [
    "tuned_history = tuned_vgg19.fit(train_images, y_train, epochs=20, validation_data=(val_images, y_val), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 84s 3s/step - loss: 1.0130 - accuracy: 0.6088\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0130324363708496, 0.6087962985038757]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_vgg19.evaluate(test_images, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 109s 4s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3.6889080e-02, 5.9533417e-01, 6.5580695e-03, 3.2265353e-01,\n",
       "        3.8564950e-02],\n",
       "       [3.2345644e-01, 1.8740186e-02, 5.6533940e-02, 1.9832566e-01,\n",
       "        4.0294373e-01],\n",
       "       [5.0349259e-03, 9.1640581e-04, 2.6032805e-01, 9.8380903e-03,\n",
       "        7.2388250e-01],\n",
       "       ...,\n",
       "       [8.3159044e-02, 3.4661740e-02, 2.7340946e-01, 3.0884725e-01,\n",
       "        2.9992247e-01],\n",
       "       [1.6272813e-03, 7.0567730e-05, 1.3759729e-01, 2.4814412e-04,\n",
       "        8.6045671e-01],\n",
       "       [5.4612823e-02, 1.3334133e-02, 5.1244223e-01, 8.5116506e-02,\n",
       "        3.3449435e-01]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_vgg19.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CNN-LABB-XOy-6h4W",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
